---
author:
  - Seth Friesz
  - Garret Spindler
title: "Final Project, STAT 600 2023"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(ggplot2)

cbPalette <- c("#999999", "#E69F00", "#56B4E9", "#009E73", "#0072B2", "#D55E00", "#F0E442","#CC79A7","#000000","#734f80", "#2b5a74", "#004f39", "#787221", "#003959", "#6aaf00", "#663cd3")
```

# Instructions

## Overview

I gave a presentation on making causal inferences about the relationship among grain yield and seeding rate, and historical grain yields. See the PDF in D2L labeled `Causal Inference`. At the time, I only had data through 2019. I want to update this analysis with data from 2020.

Suppose we have yield data from a corn field. When the corn seed was planted, it was planted at different seed densities in different parts of the field. The seeding density was targeted to be higher in the parts of the field that were expected to be more fertile. How is fertility determined? We can estimate fertility based on the past years yield over different parts of the field.

In one field, corn was planted in 2018, following soybeans in 2017. Some parts of the field had high corn yields, some parts of the field had low corn yields. Some parts of the field were planted with more seed, some parts were planted with less seed. But the parts of the field planted with low seed in 2018 were also low yielding in 2017; that's why they were planted with less seed. 

So here is a dilemma. If a specific region of the corn field was low yielding in 2018, was it because it was seeded at a lower rate, or because it is just a poor part of the field, as indicated by yield 2017?


To study this, I need to combine individual yield and seeding maps (data files with yield and seeding information tagged with a GPS position). Your task is to start with 6 files, aggregate the data by first creating a set of grid cells, then merging the data by grid cell into a single data set for further analysis. The data files, crop, year and data column to be aggregated are listed below. 

File Name                     | Crop    | Year | Original Variable   | Aggregated Variable
------------------------------|---------|------|---------------------|---------------------
`A 2017 Soybeans Harvest.csv` | Soybean | 2017 | `Yield`             | `Y17`
`A 2018 Corn Seeding.csv`     | Corn    | 2018 | `AppliedRate`       | `AR18`
`A 2018 Corn Harvest.csv`     | Corn    | 2018 | `Yield`             | `Y18`
`A 2019 Soybeans Harvest.csv` | Soybean | 2019 | `Yield`             | `Y19`
`A 2020 Corn Seeding.csv`     | Corn    | 2020 | `AppliedRate`       | `AR20`
`A 2020 Corn Harvest.csv`     | Corn    | 2020 | `Yield`             | `Y20`

## Read data
```{python}
import pandas as pd


```

## Create a grid

I would like the data to be summarized with a spatial grid of 50 m by 50 m.

Create a `Row` variable calculated from `Latitude`, such that grid cell 1 is associated with data rows with 0<`Latitude`<=50, grid cell 2 is associated with 50<`Latitude`<=100. The simplest way to do this is to divide Latitude by 50 and use the ceiling function. Similarly, create a `Column` variable from `Longitude`. 

## Aggregate data

Each combination of Row and Column will uniquely identify grid cells. Use these to aggregate the data. Compute a mean for each grid cell. Also the number of observations per grid cell.


## Merge data

Combine the individual aggregated data sets into a single data table. The rows from each table should be merged by `Cell`. Use only the aggregated cells that have at least 30 observations.


## Visualize the merged data frame

Use a pairs plot or similar visualization tool to show the relationship among the data columns in the combined data set.

## Final data format

If you name the final data table `Combined`, you will be able to run the code below to create a directed acyclic graph, appropriate for causal analysis. You do not need to run or interpret the analysis, but you should confirm that you've formatted the data properly.

If you use Python or Julia, you will need to export `Combined` to a `CSV` file, then import the datat into R to run the following code. I've not yet explored Bayesian networks in Python or Julia.

```{r,eval=FALSE}
library(bnlearn)
modela.dag <- model2network("[Y17][AR18|Y17][Y18|AR18:Y17]")
fita = bn.fit(modela.dag, Combined[,c('Y17','AR18','Y18')])

strengtha <- arc.strength(modela.dag, Combined[,c('Y17','AR18','Y18')])
strength.plot(modela.dag, strengtha)

modelb.dag <- model2network("[Y19][AR20|Y19][Y20|AR20:Y19]")
fitv = bn.fit(modelb.dag, Combined[,c('Y19','AR20','Y20')])

strengthb <- arc.strength(modelb.dag, Combined[,c('Y19','AR20','Y20')])
strength.plot(modelb.dag, strengthb)

modelc.dag <- model2network("[Y17][AR18|Y17][Y18|AR18:Y17][Y19|Y17:AR18:Y18][AR20|Y19][Y20|AR20:Y19]")
fitc = bn.fit(modelc.dag, Combined[,c('Y17','AR18','Y18','Y19','AR20','Y20')])

strengthc <- arc.strength(modelc.dag, Combined[,c('Y17','AR18','Y18','Y19','AR20','Y20')])
strength.plot(modelc.dag, strengthc)
```


## Normalize the data

There are three different units in these data. Files `A 2017 Soybeans Harvest.csv` and `A 2019 Soybeans Harvest.csv` are soybean harvests (usually around 60 bu/acre), `A 2018 Corn Harvest.csv` and `A 2020 Corn Harvest.csv` are corn harvests (usually around 180 bu/acre), while `A 2018 Corn Seeding.csv` and `A 2020 Corn Seeding.csv` contain seeding rate data. We want to normalize each of these, so that they can be more fairly compared. There may also be outliers in these data; normalization may reduce the impact of the outliers on the analysis.

Repeat the process above, but this time, normalize the data by one of the methods below before aggregating and merging the data.

Denote the $i^{th}$ `Yield` observation for Year $j$ as $y_{ij}$, we normalize yield by one of the following methods, in each case holding $j$ constant and iterating over $i$ only within years. If we assume 20 rows and 6 columns, then $y_{ij} = \left\{y_{1j},y_{2j}, \dots , y_{N_ij} \right\}$ where $N_i=120$. Similarly, we would denote the successive yield estimates for grid cell $i$ as $y_{ij} = \left\{y_{i1},y_{i2}, \dots , y_{iN_j} \right\}$ where $N_j=5$. 

Note that we do not have an index for the yield samples within each cell. You may, but are not required, compare normalization of the grid cell estimates with normalization of the yield sample values.

You may choose a normalization method at your discretion. I've listed some possible normalization formula below. You are not required to implement all three, but you must use some method to convert yield or seeding rates to a common scale. You may choose to compare the different methods; they have different statistical properties and may lead to different conclusions.

### Option 1. Rank

Replace $y_{ij}$ with $r_{ij} = \text{rank}(y_{ij})$. Determine ranks independently for $j = 1,2, \dots,N_j$ for each original data column.

### Option 2. Z-score

Calculate 

$$
\overline{y}_{. j} = \frac{\sum_{i=1}^{N_i} y_{ij}}{N_i}
$$
and 

$$
s^2_{. j} = \frac{\sum_{i=1}^{N_i} (y_{ij}-\overline{y}_{. j})^2}{N_i-1}
$$
where $N_i$ are the number of `Yield` values for year $j$. Replace $y_{ij}$ with 

$$
z_{ij} = \frac{y_{ij} -\overline{y}_{. j} }{s_{. j}}
$$. 

Calculate $\overline{y}_{. j}$ and $s^2_{. j}$ independently for $j = 1,2, \dots,N_j$ for each original data column. Note that this method makes use of the first (mean) and second moments (variance). It would be best practice to check for skewness or kurtosis of these data.


### Option 3. Percent

Replace $y_ij$ with 

$$
100 \times \frac{y_{ij}}{\overline{y}_{. j}}
$$ 
Calculate $\overline{y}_{. j}$ independently for $j = 1,2, \dots,N_j$ for each original data column. Note that this method assume the arithmetic mean is a reasonable estimate of central tendency. It would be best practice to check for skewness or kurtosis of these data.

## Visualize the normalized data frame.

Use a pairs plot or similar visualization tool to show the how normalization may affected the relationship among the data columns in the combined data set. You may also repeated the causal analysis from the first section, although this is not required.


# Grading

You will turn in a report documenting your method for merging data and your choice for normalization, but you may organize the report as you wish. You will be allowed to collaborate with one other student on this report. If you do, make sure you include both collaborators names in the final report and how each collaborator contributed to the final work.

Your report should address the following areas:

## Introduction

Start with a brief description of the problem. This will help you organize yourself for the coding. You should restate the problem in you own words; this will help you organize your thinking about the coding.


## Data

We should also include a brief description of the data and what is expected to be in the files. 

## Formula

For reproducibility, we should be specific about the normalization formula. We have different options for normalization, provide a specific definition and why you chose this method.

Writing the formula explicitly in the document should also help you organize your coding and can be used to clarify your process. We should be practicing literate documentation for reproducible research, this will include the maths.

## Algorithm

It would be best practice to write a brief summary paragraph of your algorithm, this will help organize your coding. A description of your process should follow from the description of the problem, the formulas to be applied and they data to be used. 

## Code

After you've described your process, coding should be relatively straight forward. Include comments in your code describing how the code implements the previously described algorithm.

It would simplify the document if you write loops or functions to repeat the analysis for each data set instead of copy-paste code.

## Output

Organize your output as graphs or tables in a single section so it is easier to see how your code solved the original problem.

## Conclusion

How does your output answer the original problem? Include a summary statement or paragraph; not just the printed output.

